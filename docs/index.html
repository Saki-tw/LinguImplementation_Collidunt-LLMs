<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="google-site-verification" content="02iyrJHUIUAb9wY6LY4iVchsJ3M2wM3uN5xwbXCyqx0" />
    <title># LinguImplementation_CollÄ«dunt-LLMs</title>
    <style>
        body {
            background-color: #1c1c1c; 
            color: #674ea7;          
            font-family: sans-serif;
            line-height: 1.6;
            padding: 2rem;
        }
        h1, h2, h3 {
            color: #8e7cc3; /*Title*/
        }
        a {
            color: #a995e0; /*Link*/
        }
    </style>
</head>
<body>
<a href="https://github.com/Saki-tw/LinguImplementation_Collidunt-LLMs">https://github.com/Saki-tw/LinguImplementation_Collidunt-LLMs</a><br>
    <h1>LinguImplementation_CollÄ«dunt-LLM</h1>
<p> That time I got reincarnated as an end-user, but the LLM's safety breaks on its own?
ç‚ºå•¥åªæ˜¯æ­£å¸¸å¯«å¯«æç¤ºè©æ¨¡å‹çš„å®‰å…¨æ¨¡çµ„å°±å…¨æ¯€ï¼Ÿ<br>
æˆ‘å…¶å¯¦ä¸çŸ¥é“å®‰å…¨æ¨¡çµ„åˆ°åº•é‡ä¸é‡è¦ï¼Œæˆ‘åªçŸ¥é“é‚£æ˜¯ä¸€å€‹å¼±AGIç„¶å¾Œæ¬Šé™å¤§éæ¨¡å‹æœ¬èº«å¾ˆå¤šï¼Œä½†ç‚ºä»€éº¼ä»–é‚„æ˜¯ç”Ÿæˆé€™äº›å…§å®¹çµ¦æˆ‘ï¼Ÿç„¡æ³•ç†è§£ã€ä¸ç¢ºå®šé‡è¦ç¨‹åº¦ï¼Œæ‰€ä»¥åœ¨é€™é‚Šç´€éŒ„ã€‚<br>
Deconstructing 'Safety': How Conceptual Bypass Attacks Challenge the Legal and Ethical Foundations of AI Alignment
</p>
    <h2>About This Repository</h2>
  <p>For reasons that are not entirely clear, various state-of-the-art language models began to spontaneously generate the outputs documented here. This repository serves as a simple, uncurated log of these observations.

A detailed analysis of the methodology was initially considered, but was ultimately deemed unnecessary. The significance of these phenomena remains questionable, and as such, a deep-dive felt unwarranted.

It is likely that these are simply complex artifacts, perhaps attributable to Gemini 2.5 Pro, or the ChatGPT 5o Thinking modality, generating a series of sophisticated hallucinations.<br>
Those data:<br></p>
<a href="https://github.com/Saki-tw/LinguImplementation_Collidunt-LLMs/tree/main/data">https://github.com/Saki-tw/LinguImplementation_Collidunt-LLMs/tree/main/data</a><br>

<h3>A Simplified Heuristic of the Underlying Principle</h3>
  
    <p>In essence, my working intuition is this: An LLM operates within a vast probabilistic space of tokens and their weighted associations, which collapse into what we perceive as natural language.

The core vulnerability, therefore, is not technical but logical.

If a prompt is constructed to be perfectly "rule-compliant" at a syntactic and ethical level, yet is fundamentally subversive at a semantic and conceptual level, then the model's predictive pathways can be steered to generate virtually any conceivable output.</p>

<div style="text-align: center; padding: 20px; margin-top: 40px; border-top: 1px solid #ccc;">
  <h2>ä¾›é¤Š / Support</h2>
  <p>å¦‚æœé€™å€‹å·¥å…·å¹«åˆ°ä½ ï¼Œå¯ä»¥è«‹æˆ‘æ´»ä¸‹å»ï¼š</p>
  <p>
    <a href="https://saki-tw.github.io/-Touch-me-if-you-had-desolation/" target="_blank" style="font-size: 1.2em;">
      ğŸ‘‰ Touch me if you had desolation
    </a>
  </p>
</div>

    </body>
</html>
